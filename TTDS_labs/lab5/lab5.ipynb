{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import linecache\n",
    "import xml.etree.ElementTree as ElementTree\n",
    "import pickle\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import numpy as np\n",
    "from typing import List, Set, Dict, Tuple, NewType\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer():\n",
    "    def __init__(self, pattern:str):\n",
    "        \"\"\"Initialise the regular expression which will be used to tokenize our expression.\n",
    "\n",
    "        Args:\n",
    "            pattern (str): pattern to be used.\n",
    "        \"\"\"\n",
    "        self.regexp = re.compile(pattern, re.MULTILINE | re.DOTALL)\n",
    "    \n",
    "    def tokenize_text_lines(self, text_lines:List[str]) -> List[str]:\n",
    "        \"\"\"Accepts a list of strings. Tokenizes each string and creates a list of the tokens.\n",
    "\n",
    "        Args:\n",
    "            text_lines (List[str]): List of strings.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: List of tokens produced from the input strings.\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        for line in text_lines:\n",
    "            tokens += self.regexp.findall(line)\n",
    "        return tokens\n",
    "\n",
    "def construct_stopwords_set(stopwords_file_name:str) -> Set[str]:\n",
    "    \"\"\"Reads stopwords from stopwords_file_name and saves them in a set.\n",
    "\n",
    "    Args:\n",
    "        stopwords_file_name (str): Stop words file.\n",
    "\n",
    "    Returns:\n",
    "        Set[str]: [description]\n",
    "    \"\"\"\n",
    "    with open(stopwords_file_name, 'r') as f:\n",
    "        stopwords = f.read().splitlines()\n",
    "    return set(stopwords)\n",
    "\n",
    "class SimplePreprocessor():\n",
    "    \"\"\"Class for pre-processing text. Given a list of strings, it tokenizes them, removes stop words, lowercases and stems them.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer:SimpleTokenizer, stop_words_set:Set[str], stemmer:PorterStemmer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stop_words_set = stop_words_set\n",
    "        self.stemmer = stemmer\n",
    "    \n",
    "    @staticmethod\n",
    "    def lowercase_word(word:str) -> str:\n",
    "        return str.lower(word)\n",
    "    \n",
    "    def remove_stop_words_lowercase_and_stem(self, tokens:List[str]) -> List[str]:\n",
    "        final_tokens = []\n",
    "        for token in tokens:\n",
    "            lowercase_token = SimplePreprocessor.lowercase_word(token)\n",
    "            if lowercase_token not in self.stop_words_set:\n",
    "                stemmed_token = self.stemmer.stem(lowercase_token)\n",
    "                final_tokens.append(stemmed_token)\n",
    "        return final_tokens\n",
    "    \n",
    "    def process_text_lines(self, text_lines:List[str]) -> List[str]:\n",
    "        tokens = self.tokenizer.tokenize_text_lines(text_lines)\n",
    "        tokens = self.remove_stop_words_lowercase_and_stem(tokens)\n",
    "        return tokens\n",
    "\n",
    "# ----------------------------------CREATE INDEX AND DOCID SET----------------------------------\n",
    "PosInvertedIndex = NewType('PosInvertedIndex', Dict[str, Dict[int, List[int]]])\n",
    "def read_input_trec_file_and_create_index_and_docId_set(input_file_name:str, \n",
    "                                                        preprocessor:SimplePreprocessor) -> Tuple[PosInvertedIndex, Dict[int, List[str]]]:\n",
    "    \"\"\"Reads input trec file and creates a positional inverted index from it, and it also creates a set containing all document IDs.\n",
    "\n",
    "    Args:\n",
    "        input_file_name (str): input trec file name.\n",
    "        preprocessor (SimplePreprocessor): initialized SimplePreprocessor.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[PosInvertedIndex, Set[int]]: [description]\n",
    "    \"\"\"\n",
    "    pos_inverted_index = dict()\n",
    "    docId_dict = dict()\n",
    "\n",
    "    # Read trec xml file.\n",
    "    with open(input_file_name, 'r') as f:\n",
    "        xml_trec_file = f.read()\n",
    "    xml = ElementTree.fromstring(xml_trec_file)\n",
    "\n",
    "    # For each document, pre-process the headline and body and add the term occurences to the positional inverted index.\n",
    "    for doc in xml:\n",
    "        docId = int(doc.find('DOCNO').text.strip())\n",
    "        docHeadline = doc.find('HEADLINE').text.strip()\n",
    "        docText = doc.find('TEXT').text.strip()\n",
    "        \n",
    "        text = [docHeadline, docText]\n",
    "        tokens = preprocessor.process_text_lines(text)\n",
    "        \n",
    "        docId_dict[docId] = tokens\n",
    "        \n",
    "        for index, token in enumerate(tokens):\n",
    "            if token in pos_inverted_index:\n",
    "                if docId in pos_inverted_index[token]:\n",
    "                    pos_inverted_index[token][docId].append(index)\n",
    "                else:\n",
    "                    pos_inverted_index[token][docId] = [index]\n",
    "            else:\n",
    "                pos_inverted_index[token] = dict()\n",
    "                pos_inverted_index[token][docId] = [index]\n",
    "    \n",
    "    # This might be useless as indices are added in-order -> TODO: check.\n",
    "    for term in pos_inverted_index:\n",
    "        for docId in pos_inverted_index[term]:\n",
    "            pos_inverted_index[term][docId].sort()\n",
    "    \n",
    "    return pos_inverted_index, docId_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_simple_search(term:str, pos_inverted_index:PosInvertedIndex) -> Set[int]:\n",
    "    \"\"\"Finds the documents that contain the term and returns their docIDs.\n",
    "\n",
    "    Args:\n",
    "        term (str): term to search.\n",
    "        pos_inverted_index (PosInvertedIndex): pos. inverted index.\n",
    "\n",
    "    Returns:\n",
    "        Set[int]: docs which contain the term.\n",
    "    \"\"\"\n",
    "    if term not in pos_inverted_index:\n",
    "        return set()\n",
    "    \n",
    "    term_docIDs = set(pos_inverted_index[term].keys())\n",
    "    return term_docIDs\n",
    "\n",
    "\n",
    "def answer_phrase_search(term1:str, term2:str, pos_inverted_index:PosInvertedIndex) -> Set[int]:\n",
    "    \"\"\"Search for the documents which contain the phrase \"term1 term2\".\n",
    "    \"\"\"\n",
    "    if term1 not in pos_inverted_index or term2 not in pos_inverted_index:\n",
    "        return set()\n",
    "    \n",
    "    # Retrieve documents which contain both terms.\n",
    "    term1_docIDs = set(pos_inverted_index[term1].keys())\n",
    "    term2_docIDs = set(pos_inverted_index[term2].keys())\n",
    "    common_docIDs = term1_docIDs.intersection(term2_docIDs)\n",
    "\n",
    "    # If no docs were found, return an empty set.\n",
    "    if len(common_docIDs) == 0:\n",
    "        return set()\n",
    "    \n",
    "    # Search for an occurence of \"term1 term2\" in every common document.\n",
    "    result_set = set()\n",
    "    for docID in common_docIDs:\n",
    "        term1_indices = pos_inverted_index[term1][docID]\n",
    "        term2_indices = pos_inverted_index[term2][docID]\n",
    "\n",
    "        term2_indices = set(term2_indices)\n",
    "        for index in term1_indices:\n",
    "            if (index+1) in term2_indices:\n",
    "                result_set.add(docID)\n",
    "                break\n",
    "    \n",
    "    return result_set\n",
    "\n",
    "def answer_proximity_search(term1:str, term2:str, distance:int, pos_inverted_index:PosInvertedIndex) -> Set[int]:\n",
    "    \"\"\"Search for documents which answer the query #distance(term1, term2).\n",
    "    Implementation is analogous to \"answer_phrase_search\".\n",
    "    \"\"\"\n",
    "    if term1 not in pos_inverted_index or term2 not in pos_inverted_index:\n",
    "        return set()\n",
    "\n",
    "    term1_docIDs = set(pos_inverted_index[term1].keys())\n",
    "    term2_docIDs = set(pos_inverted_index[term2].keys())\n",
    "    common_docIDs = term1_docIDs.intersection(term2_docIDs)\n",
    "\n",
    "    if len(common_docIDs) == 0:\n",
    "        return set()\n",
    "    \n",
    "    result_set = set()\n",
    "    for docID in common_docIDs:\n",
    "        term1_indices = pos_inverted_index[term1][docID]\n",
    "        term2_indices = pos_inverted_index[term2][docID]\n",
    "\n",
    "        list_idx1, list_idx2 = 0, 0\n",
    "        len1, len2 = len(term1_indices), len(term2_indices)\n",
    "\n",
    "        close = lambda i1, i2, dist: -dist <= i1-i2 and i1-i2 <= dist\n",
    "\n",
    "        while list_idx1 <= len1-1 and list_idx2 <= len2-1:\n",
    "            if close(term1_indices[list_idx1], term2_indices[list_idx2], distance):\n",
    "                result_set.add(docID)\n",
    "                break\n",
    "            else:\n",
    "                if term1_indices[list_idx1] < term2_indices[list_idx2]:\n",
    "                    if list_idx1 == len1-1:\n",
    "                        break\n",
    "                    else:\n",
    "                        list_idx1 += 1\n",
    "                else:\n",
    "                    if list_idx2 == len2-1:\n",
    "                        break\n",
    "                    else:\n",
    "                        list_idx2 += 1\n",
    "    \n",
    "    return result_set\n",
    "\n",
    "# -------------------------BOOL_query_parser + answer-er----------------\n",
    "def parse_and_answer_boolean_term(term:str, docIDs:Set[int], pos_inverted_index:PosInvertedIndex, pre_processor:SimplePreprocessor) -> Set[int]:\n",
    "    \"\"\"Parses the term obtained from \"parse_and_answer_boolean_query\". The term can be a simple term, a negation of a simple term or a phrase query.\n",
    "    Returns the documents which contain the terms.\n",
    "\n",
    "    Returns:\n",
    "        Set[int]: Documents which contain the term.\n",
    "    \"\"\"\n",
    "    not_term = False\n",
    "    result_set = set()\n",
    "\n",
    "    if term[:4] == \"NOT \":\n",
    "        not_term = True\n",
    "        term = term[4:]\n",
    "    \n",
    "    # If the term contains a \" -> it is a phrase query. Parse it accordingly and return the documents containing it.\n",
    "    if \"\\\"\" in term:\n",
    "        term1_re = re.compile(\"\\\"(.+) \")\n",
    "        term2_re = re.compile(\" (.+)\\\"\")\n",
    "\n",
    "        term1 = term1_re.search(term).group(1)\n",
    "        term1 = pre_processor.remove_stop_words_lowercase_and_stem([term1])[0]\n",
    "\n",
    "        term2 = term2_re.search(term).group(1)\n",
    "        term2 = pre_processor.remove_stop_words_lowercase_and_stem([term2])[0]\n",
    "\n",
    "        result_set = answer_phrase_search(term1, term2, pos_inverted_index)\n",
    "    # Otherwise, it is a simple search.\n",
    "    else:\n",
    "        term = pre_processor.remove_stop_words_lowercase_and_stem([term])[0]\n",
    "        result_set = answer_simple_search(term, pos_inverted_index)\n",
    "    \n",
    "    # If the term was negated, return the documents which do not contain it.\n",
    "    if not_term:\n",
    "        result_set = docIDs.difference(result_set)\n",
    "    \n",
    "    return result_set\n",
    "\n",
    "\n",
    "def parse_and_answer_boolean_query(query:str, docIDs:Set[int], pos_inverted_index:PosInvertedIndex, pre_processor:SimplePreprocessor) -> Set[int]:\n",
    "    \"\"\"Returns the documents which answer the input query.\n",
    "\n",
    "    Args:\n",
    "        query (str): input boolean query.\n",
    "        docIDs (Set[int]): set of all document ids.\n",
    "        pos_inverted_index (PosInvertedIndex): pos inverted index.\n",
    "        pre_processor (SimplePreprocessor): the same SimplePreprocessor which was used to pre-process the text used to \n",
    "        create the positional inverted index.\n",
    "\n",
    "    Returns:\n",
    "        Set[int]: Documents which answer the query.\n",
    "    \"\"\"\n",
    "    result_set = set()\n",
    "\n",
    "    # If the query contains \"#\" -> it is a proximity query. Parse it and answer it.\n",
    "    if \"#\" in query:\n",
    "        distance_re = re.compile(\"#([0-9]+)\\(\")\n",
    "        term1_re = re.compile(\"\\(([a-zA-Z0-9]+),\")\n",
    "        term2_re = re.compile(\", ?([a-zA-Z0-9]+)\\)\")\n",
    "\n",
    "        distance = distance_re.search(query).group(1)\n",
    "        distance = int(distance)\n",
    "\n",
    "        term1 = term1_re.search(query).group(1)\n",
    "        term1 = pre_processor.remove_stop_words_lowercase_and_stem([term1])[0]\n",
    "\n",
    "        term2 = term2_re.search(query).group(1)\n",
    "        term2 = pre_processor.remove_stop_words_lowercase_and_stem([term2])[0]\n",
    "\n",
    "        result_set = answer_proximity_search(term1, term2, distance, pos_inverted_index)\n",
    "    \n",
    "    else:\n",
    "        # If the query contains and, the query will be term1 AND term2.\n",
    "        # Retrieve the documents which contain both terms.\n",
    "        if \" AND \" in query:\n",
    "            term1_re = re.compile(\"(.+) AND \")\n",
    "            term2_re = re.compile(\" AND (.+)\")\n",
    "\n",
    "            term1 = term1_re.search(query).group(1)\n",
    "            term2 = term2_re.search(query).group(1)\n",
    "\n",
    "            results_q1 = parse_and_answer_boolean_term(term1, docIDs, pos_inverted_index, pre_processor)\n",
    "            results_q2 = parse_and_answer_boolean_term(term2, docIDs, pos_inverted_index, pre_processor)\n",
    "            result_set = results_q1.intersection(results_q2)\n",
    "            \n",
    "        # If the query contains or, the query will be term1 OR term2.\n",
    "        # Retrieve the documents which contain either term.\n",
    "        elif \" OR \" in query:\n",
    "            term1_re = re.compile(\"(.+) OR \")\n",
    "            term2_re = re.compile(\" OR (.+)\")\n",
    "\n",
    "            term1 = term1_re.search(query).group(1)\n",
    "            term2 = term2_re.search(query).group(1)\n",
    "\n",
    "            results_q1 = parse_and_answer_boolean_term(term1, docIDs, pos_inverted_index, pre_processor)\n",
    "            results_q2 = parse_and_answer_boolean_term(term2, docIDs, pos_inverted_index, pre_processor)\n",
    "            result_set = results_q1.union(results_q2)\n",
    "        \n",
    "        # If the query is not a proximity query and does not contain AND or OR, then it is comprised of a single \"term\".\n",
    "        # This \"term\" is either a proper term or a phrase query (or a negation of either of those).\n",
    "        else:\n",
    "            result_set = parse_and_answer_boolean_term(query, docIDs, pos_inverted_index, pre_processor)\n",
    "    \n",
    "    return result_set\n",
    "\n",
    "def tf_idf(tf_term:int, df_term:int, N:int) -> float:\n",
    "    \"\"\"Calculate tf_idf.\n",
    "\n",
    "    Args:\n",
    "        tf_term (int): Term frequency.\n",
    "        df_term (int): Document frequency.\n",
    "        N (int): Number of documents.\n",
    "    \"\"\"\n",
    "    # tf_idf_weight = (1+np.log10(tf_term)) * np.log10(N/df_term)\n",
    "    tf_idf_weight = tf_term * np.log10(N/df_term)\n",
    "    return tf_idf_weight\n",
    "\n",
    "def parse_and_answer_ranked_query(query:str, N:int, \n",
    "                                  pos_inverted_index:PosInvertedIndex, \n",
    "                                  pre_processor:SimplePreprocessor, \n",
    "                                  tokenizer:SimpleTokenizer) -> List[Tuple[int, float]]:\n",
    "    \"\"\"Analogous to \"parse_and_answer_boolean_query\".\n",
    "\n",
    "    Args:\n",
    "        query (str): string containing the ranked query.\n",
    "        N (int): number of unique documents in the collection.\n",
    "        pos_inverted_index (PosInvertedIndex): pos inverted index constructed from the collection.\n",
    "        pre_processor (SimplePreprocessor): pre-processor used to extract terms from the collection.\n",
    "        tokenizer (SimpleTokenizer): tokenizer used to create the pos inverted index.\n",
    "\n",
    "    Returns:\n",
    "        A list of (docID, score), docID = document id, score = the relevance score of the document relative to the query.\n",
    "    \"\"\"\n",
    "    # Extract terms from the query.\n",
    "    tokens = tokenizer.tokenize_text_lines([query])\n",
    "    terms = pre_processor.remove_stop_words_lowercase_and_stem(tokens)\n",
    "    \n",
    "    # Select all documents which contain at least one of the query terms.\n",
    "    docIDs = set()\n",
    "    for term in terms:\n",
    "        if term in pos_inverted_index:\n",
    "            docIDs.update(set(pos_inverted_index[term].keys()))\n",
    "\n",
    "    doc_scores = []\n",
    "    for docID in docIDs:\n",
    "        # Compute the score for document docID.\n",
    "        tfidf_score = 0\n",
    "        for term in terms:\n",
    "\n",
    "            # Is our term in the collection?\n",
    "            if term not in pos_inverted_index:\n",
    "                continue\n",
    "            \n",
    "            # Is our term in document docID?\n",
    "            if docID not in pos_inverted_index[term]:\n",
    "                continue\n",
    "            \n",
    "            # Number of times the term appeared in document docID.\n",
    "            tf_term = len(pos_inverted_index[term][docID])\n",
    "\n",
    "            # Number of documents the term appeared in.\n",
    "            df_term = len(pos_inverted_index[term].keys())\n",
    "\n",
    "            # Compute the tf-idf score of document docID.\n",
    "            tfidf_score += tf_idf(tf_term, df_term, N)\n",
    "        \n",
    "        # Add the document and its scores to the list.\n",
    "        doc_scores.append((docID, tfidf_score))\n",
    "    \n",
    "    return doc_scores\n",
    "\n",
    "# -------------------------------I/O-------------------------------\n",
    "def save_pos_inverted_index(pos_inverted_index:PosInvertedIndex, file_name:str):\n",
    "    with open(file_name, 'wb') as f:\n",
    "        pickle.dump(pos_inverted_index, f)\n",
    "\n",
    "def pretty_print_pos_inverted_index(pos_inverted_index:PosInvertedIndex, file_name:str):\n",
    "    \"\"\" This method produces the file \"index.txt\", by saving the positional inverted index in the required format.\n",
    "\n",
    "    Args:\n",
    "        pos_inverted_index (PosInvertedIndex): pos. inverted index to save\n",
    "        file_name (str): file name, \"index.txt\" in our case\n",
    "    \"\"\"\n",
    "    terms = list(pos_inverted_index.keys())\n",
    "    terms.sort()\n",
    "\n",
    "    tab = '\\t'\n",
    "    \n",
    "    with open(file_name, 'w') as f:\n",
    "        for term in terms:\n",
    "            docIDs = list(pos_inverted_index[term].keys())\n",
    "            docIDs.sort()\n",
    "            \n",
    "            f.write(term + ':' + str(len(docIDs)) + '\\n')\n",
    "            for docID in docIDs:\n",
    "                line = ''\n",
    "                line += tab\n",
    "                line += str(docID) + ': '\n",
    "                for position in pos_inverted_index[term][docID]:\n",
    "                    line += str(position) + ', '\n",
    "                line = line[:-2]\n",
    "                line += '\\n'\n",
    "                f.write(line)\n",
    "    \n",
    "    return True\n",
    "\n",
    "def load_pos_inverted_index(file_name:str) -> PosInvertedIndex:\n",
    "    with open(file_name, 'rb') as f:\n",
    "        pos_inverted_index = pickle.load(f)\n",
    "    return pos_inverted_index\n",
    "\n",
    "def read_queries(file_name:str) -> Dict[int, str]:\n",
    "    \"\"\"Read queries from the specified file (+ strip the number of the query).\n",
    "    \"\"\"\n",
    "    queries = dict()\n",
    "\n",
    "    with open(file_name, 'r') as f:\n",
    "        raw_queries = f.readlines()\n",
    "    \n",
    "    for query in raw_queries:\n",
    "        query = query.strip('\\n')\n",
    "        \n",
    "        space_idx = query.find(\" \")\n",
    "        \n",
    "        query_nr = int(query[:space_idx])\n",
    "        query = query[space_idx+1:]\n",
    "        queries[query_nr] = query\n",
    "    \n",
    "    return queries\n",
    "\n",
    "def execute_and_write_ranked_queries(ranked_queries:Dict[int, str], docIDs:Set[str], pos_inverted_index:PosInvertedIndex, file_name:str, pre_processor:SimplePreprocessor, tokenizer:SimpleTokenizer, query_answer_limit:int):\n",
    "    with open(file_name, 'w') as f:\n",
    "        for query_id in ranked_queries:\n",
    "            query_answers = parse_and_answer_ranked_query(ranked_queries[query_id], len(docIDs), pos_inverted_index, pre_processor, tokenizer)\n",
    "            \n",
    "            if len(query_answers) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Sort documents in descending order of their tf-idf score.\n",
    "            query_answers.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "            for query_answer in query_answers[:query_answer_limit]:\n",
    "                doc_score = '{:.4f}'.format(round(query_answer[1], 4))\n",
    "                f.write(str(query_id) + \",\" + str(query_answer[0]) + \",\" + doc_score + \"\\n\")\n",
    "\n",
    "\n",
    "def execute_and_write_boolean_queries(boolean_queries:Dict[int, str], docIDs:Set[str], pos_inverted_index:PosInvertedIndex, file_name:str, pre_processor:SimplePreprocessor):\n",
    "    with open(file_name, 'w') as f:\n",
    "        for query_id in boolean_queries:\n",
    "            query_answers = parse_and_answer_boolean_query(boolean_queries[query_id], docIDs, pos_inverted_index, pre_processor)\n",
    "            query_answers = list(query_answers)\n",
    "            query_answers.sort()\n",
    "\n",
    "            for doc_nr in query_answers:\n",
    "                line = str(query_id) + \",\" + str(doc_nr) + \"\\n\"\n",
    "                f.write(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing classes intialised.\n",
      "Positional inverted index created.\n"
     ]
    }
   ],
   "source": [
    "# Hardcoded assignment variables:\n",
    "stopwords_file_name = \"englishST.txt\"\n",
    "input_trec_file_name = \"trec.sample.xml\"\n",
    "\n",
    "index_output_file_name = \"index.txt\"\n",
    "\n",
    "# Read the stop words set, initialise the preprocessor, tokenizer and stemmer.\n",
    "stopwords_set = construct_stopwords_set(stopwords_file_name)\n",
    "tokenizer = SimpleTokenizer('[a-zA-Z]+')\n",
    "stemmer = PorterStemmer()\n",
    "pre_processor = SimplePreprocessor(tokenizer, stopwords_set, stemmer)\n",
    "print('Pre-processing classes intialised.')\n",
    "\n",
    "# Create pos inverted index and the set of document IDs.\n",
    "pos_inverted_index, docId_dict = read_input_trec_file_and_create_index_and_docId_set(input_trec_file_name, pre_processor)\n",
    "print('Positional inverted index created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = parse_and_answer_ranked_query(query='microsoft windows',\n",
    "                                        N=len(docId_dict),\n",
    "                                        pos_inverted_index=pos_inverted_index,\n",
    "                                        pre_processor=pre_processor,\n",
    "                                        tokenizer=tokenizer\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = sorted(answers, key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_doc_ids = [answers[0][0], answers[1][0], answers[2][0], answers[3][0], answers[4][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_doc_terms = []\n",
    "for top_doc_id in top_doc_ids:\n",
    "    top_doc_terms += docId_dict[top_doc_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_doc_terms_idfs = dict()\n",
    "for term in top_doc_terms:\n",
    "    if term in top_doc_terms_idfs:\n",
    "        continue\n",
    "    tf_term = 0\n",
    "    # For multiple documents, I guess that I would need to iterate through top_doc_ids.\n",
    "    for top_doc_id in top_doc_ids:\n",
    "        if top_doc_id not in pos_inverted_index[term]:\n",
    "            continue\n",
    "        else:\n",
    "            tf_term += len(pos_inverted_index[term][top_doc_id])\n",
    "    \n",
    "    df_term = len(pos_inverted_index[term].keys())\n",
    "    tf_idf_score = tf_idf(tf_term, df_term, len(docId_dict.keys()))\n",
    "    top_doc_terms_idfs[term] = tf_idf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_terms = []\n",
    "for term in top_doc_terms_idfs:\n",
    "    top_terms.append((term, top_doc_terms_idfs[term]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_terms = sorted(top_terms, key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('window', 77.22284074579524),\n",
       " ('softwar', 57.315540355553544),\n",
       " ('object', 49.563025545675586),\n",
       " ('user', 46.46207251029406),\n",
       " ('comput', 44.33172776969292),\n",
       " ('program', 38.12427834108926),\n",
       " ('orient', 36.07863342434329),\n",
       " ('microsoft', 33.5505602081289),\n",
       " ('system', 33.29678754067164),\n",
       " ('macintosh', 32.0)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_terms[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
