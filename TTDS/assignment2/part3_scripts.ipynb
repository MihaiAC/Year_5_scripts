{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from typing import List, Set, Dict, Tuple, NewType\n",
    "from scipy.sparse import dok_matrix\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BOW():\n",
    "    def __init__(self):\n",
    "        self.token_to_id = dict()\n",
    "        self.id_to_token = dict()\n",
    "    \n",
    "    @property\n",
    "    def nr_tokens(self) -> int:\n",
    "        return len(self.token_to_id)\n",
    "    \n",
    "    def transform_tokenized_sent(self, tokenized_sent:List[str]) -> List[int]:\n",
    "        output = []\n",
    "        for token in tokenized_sent:\n",
    "            output.append(self.token_to_id[token])\n",
    "        return output\n",
    "    \n",
    "    def transform_tokenized_sents(self, tokenized_sents:List[List[str]]) -> List[List[int]]:\n",
    "        output = []\n",
    "        for sent in tokenized_sents:\n",
    "            self.add_list_of_tokens(sent)\n",
    "            output.append(self.transform_tokenized_sent(sent))\n",
    "        return output\n",
    "    \n",
    "    def reverse_transform_sent(self, id_sent:List[int]) -> List[str]:\n",
    "        output = []\n",
    "        for token_id in id_sent:\n",
    "            output.append(self.id_to_token[token_id])\n",
    "        return output\n",
    "    \n",
    "    def reverse_transform_sents(self, id_sents:List[List[str]]) -> List[List[int]]:\n",
    "        output = []\n",
    "        for sent in id_sents:\n",
    "            output.append(self.reverse_transform_sent(sent))\n",
    "        return output\n",
    "        \n",
    "    def add_token(self, token:str):\n",
    "        if token not in self.token_to_id:\n",
    "            nr_tokens = len(self.token_to_id)\n",
    "            self.token_to_id[token] = nr_tokens\n",
    "            self.id_to_token[nr_tokens] = token\n",
    "    \n",
    "    def add_list_of_tokens(self, tokens:List[str]):\n",
    "        for token in tokens:\n",
    "            self.add_token(token)\n",
    "            \n",
    "    def get_token_id(self, token:str) -> int:\n",
    "        if token not in self.token_to_id:\n",
    "            print(\"Token \" + token + \" not in the BOW\")\n",
    "            return -1\n",
    "        else:\n",
    "            return self.token_to_id[token]\n",
    "    \n",
    "    def get_token_from_id(self, token_id:int) -> str:\n",
    "        if token_id not in self.id_to_token:\n",
    "            print(\"Token id \" + str(token_id) + \" is not in the BOW.\")\n",
    "            return \"\"\n",
    "        else:\n",
    "            return self.id_to_token[token]\n",
    "        \n",
    "\n",
    "class SimpleTokenizer():\n",
    "    def __init__(self, pattern:str):\n",
    "        \"\"\"Initialise the regular expression which will be used to tokenize our expression.\n",
    "\n",
    "        Args:\n",
    "            pattern (str): pattern to be used.\n",
    "        \"\"\"\n",
    "        self.regexp = re.compile(pattern, re.MULTILINE | re.DOTALL)\n",
    "    \n",
    "    def tokenize_text_lines(self, text_lines:List[str]) -> List[str]:\n",
    "        \"\"\"Accepts a list of strings. Tokenizes each string and creates a list of the tokens.\n",
    "\n",
    "        Args:\n",
    "            text_lines (List[str]): List of strings.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: List of tokens produced from the input strings.\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        for line in text_lines:\n",
    "            tokens += self.regexp.findall(line)\n",
    "        return tokens\n",
    "    \n",
    "    def tokenize_list_of_strings(self, string_list:List[str]) -> List[List[str]]:\n",
    "        list_of_tokens = []\n",
    "        for string in string_list:\n",
    "            list_of_tokens.append(self.regexp.findall(string))\n",
    "        return list_of_tokens\n",
    "\n",
    "def read_tsv_extract_corpora(tsv_file_name:str, corpus_names_to_int:Dict[str, int]) -> Dict[int, List[str]]:\n",
    "    corpora = dict()\n",
    "    for value in corpus_names_to_int.values():\n",
    "        corpora[value] = []\n",
    "    with open(tsv_file_name, mode='r', newline='\\n') as f:\n",
    "        read_tsv = csv.reader(f, delimiter=\"\\t\")\n",
    "        for row in read_tsv:\n",
    "            corpus_name = row[0]\n",
    "            corpus_id = corpus_names_to_int[corpus_name]\n",
    "            corpora[corpus_id].append(row[1])\n",
    "    return corpora\n",
    "\n",
    "def tokenize_corpora(corpora:Dict[int, List[str]], tokenizer:SimpleTokenizer) -> Dict[int, List[List[str]]]:\n",
    "    tokenized_corpora = dict()\n",
    "    for key in corpora.keys():\n",
    "        tokenized_corpora[key] = []\n",
    "        for document in corpora[key]:\n",
    "            document_terms = tokenizer.tokenize_text_lines([document])\n",
    "            tokenized_corpora[key].append(document_terms)\n",
    "    return tokenized_corpora\n",
    "\n",
    "def docs_to_bow_sents(docs:List[List[str]]) -> BOW, List[List[int]]:\n",
    "    bow = BOW()\n",
    "    bow_sents = bow.transform_tokenized_sents(docs)\n",
    "    return bow, bow_sents\n",
    "\n",
    "def bow_sents_to_dok(bow_sents:List[List[int]], bow:BOW) -> dok_matrix:\n",
    "    nr_tokens = bow.nr_tokens\n",
    "    dok = dok_matrix((len(bow_sents), nr_tokens), dtype='int')\n",
    "    for sent_number, sent in enumerate(bow_sents):\n",
    "        for token_id in sent:\n",
    "            dok[sent_number, token_id] += 1\n",
    "    return dok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Praise', 'be', 'to', 'Allah', 'Lord', 'of', 'the', 'Worlds'],\n",
       " ['the', 'Merciful', 'the', 'Most', 'Merciful'],\n",
       " ['Owner', 'of', 'the', 'Day', 'of', 'Recompense'],\n",
       " ['Guide', 'us', 'to', 'the', 'Straight', 'Path'],\n",
       " ['the',\n",
       "  'Path',\n",
       "  'of',\n",
       "  'those',\n",
       "  'upon',\n",
       "  'whom',\n",
       "  'You',\n",
       "  'have',\n",
       "  'favored',\n",
       "  'not',\n",
       "  'those',\n",
       "  'upon',\n",
       "  'whom',\n",
       "  'is',\n",
       "  'the',\n",
       "  'anger',\n",
       "  'nor',\n",
       "  'the',\n",
       "  'astray',\n",
       "  'Amen',\n",
       "  'please',\n",
       "  'answer'],\n",
       " ['AlifLaamMeem'],\n",
       " ['That',\n",
       "  'is',\n",
       "  'the',\n",
       "  'Holy',\n",
       "  'Book',\n",
       "  'where',\n",
       "  'there',\n",
       "  'is',\n",
       "  'no',\n",
       "  'doubt',\n",
       "  'It',\n",
       "  'is',\n",
       "  'a',\n",
       "  'guidance',\n",
       "  'for',\n",
       "  'the',\n",
       "  'cautious',\n",
       "  'of',\n",
       "  'evil',\n",
       "  'and',\n",
       "  'Hell'],\n",
       " ['Who',\n",
       "  'believe',\n",
       "  'in',\n",
       "  'the',\n",
       "  'unseen',\n",
       "  'and',\n",
       "  'establish',\n",
       "  'the',\n",
       "  'daily',\n",
       "  'prayer',\n",
       "  'who',\n",
       "  'spend',\n",
       "  'out',\n",
       "  'of',\n",
       "  'what',\n",
       "  'We',\n",
       "  'have',\n",
       "  'provided',\n",
       "  'them'],\n",
       " ['Who',\n",
       "  'believe',\n",
       "  'in',\n",
       "  'that',\n",
       "  'which',\n",
       "  'has',\n",
       "  'been',\n",
       "  'sent',\n",
       "  'down',\n",
       "  'to',\n",
       "  'you',\n",
       "  'Prophet',\n",
       "  'Muhammad',\n",
       "  'and',\n",
       "  'what',\n",
       "  'has',\n",
       "  'been',\n",
       "  'sent',\n",
       "  'down',\n",
       "  'before',\n",
       "  'you',\n",
       "  'to',\n",
       "  'Prophets',\n",
       "  'Jesus',\n",
       "  'and',\n",
       "  'Moses',\n",
       "  'and',\n",
       "  'firmly',\n",
       "  'believe',\n",
       "  'in',\n",
       "  'the',\n",
       "  'Everlasting',\n",
       "  'Life'],\n",
       " ['These',\n",
       "  'are',\n",
       "  'guided',\n",
       "  'by',\n",
       "  'their',\n",
       "  'Lord',\n",
       "  'these',\n",
       "  'surely',\n",
       "  'are',\n",
       "  'the',\n",
       "  'prosperous']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizer('[a-zA-Z]+')\n",
    "tokenized_sents = tokenizer.tokenize_list_of_strings(['something or other', 'other or something..'])\n",
    "bow = BOW()\n",
    "bow_sents = bow.transform_tokenized_sents(tokenized_sents)\n",
    "\n",
    "tsv_file_name = 'train_and_dev.tsv'\n",
    "stopwords_file_name = \"englishST.txt\"\n",
    "index_output_file_name = \"index.txt\"\n",
    "\n",
    "corpus_names_to_int = {'Quran':0, 'OT':1, 'NT':2}\n",
    "int_to_corpus_names = {0:'Quran', 1:'OT', 2:'NT'}\n",
    "corpora = read_tsv_extract_corpora(tsv_file_name, corpus_names_to_int)\n",
    "tokenized_corpora = tokenize_corpora(corpora, tokenizer)\n",
    "\n",
    "bow, bow_sents = docs_to_bow_sents(corpora[0].items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Praise be to Allah, Lord of the Worlds,', 'the Merciful, the Most Merciful,', 'Owner of the Day of Recompense.', 'Guide us to the Straight Path,', 'the Path of those upon whom You have favored, not those upon whom is the anger, nor the astray. (Amen please answer)', 'AlifLaamMeem.', 'That is the (Holy) Book, where there is no doubt. It is a guidance for the cautious (of evil and Hell).', 'Who believe in the unseen and establish the (daily) prayer; who spend out of what We have provided them.', 'Who believe in that which has been sent down to you (Prophet Muhammad) and what has been sent down before you (to Prophets Jesus and Moses) and firmly believe in the Everlasting Life.', 'These are guided by their Lord; these surely are the prosperous.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
