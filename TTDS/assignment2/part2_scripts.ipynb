{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import numpy as np\n",
    "from typing import List, Set, Dict, Tuple, NewType\n",
    "from operator import itemgetter\n",
    "from nltk.corpus import stopwords\n",
    "import csv\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.corpora.dictionary import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer():\n",
    "    def __init__(self, pattern:str):\n",
    "        \"\"\"Initialise the regular expression which will be used to tokenize our expression.\n",
    "\n",
    "        Args:\n",
    "            pattern (str): pattern to be used.\n",
    "        \"\"\"\n",
    "        self.regexp = re.compile(pattern, re.MULTILINE | re.DOTALL)\n",
    "    \n",
    "    def tokenize_text_lines(self, text_lines:List[str]) -> List[str]:\n",
    "        \"\"\"Accepts a list of strings. Tokenizes each string and creates a list of the tokens.\n",
    "\n",
    "        Args:\n",
    "            text_lines (List[str]): List of strings.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: List of tokens produced from the input strings.\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        for line in text_lines:\n",
    "            tokens += self.regexp.findall(line)\n",
    "        return tokens\n",
    "\n",
    "def construct_stopwords_set(stopwords_file_name:str) -> Set[str]:\n",
    "    \"\"\"Reads stopwords from stopwords_file_name and saves them in a set.\n",
    "\n",
    "    Args:\n",
    "        stopwords_file_name (str): Stop words file.\n",
    "\n",
    "    Returns:\n",
    "        Set[str]: [description]\n",
    "    \"\"\"\n",
    "    with open(stopwords_file_name, 'r') as f:\n",
    "        read_stopwords = f.read().splitlines()\n",
    "    stopwords_set = set(read_stopwords)\n",
    "    stopwords_set.update(stopwords.words(\"english\"))\n",
    "    return stopwords_set\n",
    "\n",
    "class SimplePreprocessor():\n",
    "    \"\"\"Class for pre-processing text. Given a list of strings, it tokenizes them, removes stop words, lowercases and stems them.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer:SimpleTokenizer, stop_words_set:Set[str], stemmer:PorterStemmer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stop_words_set = stop_words_set\n",
    "        self.stemmer = stemmer\n",
    "    \n",
    "    @staticmethod\n",
    "    def lowercase_word(word:str) -> str:\n",
    "        return str.lower(word)\n",
    "    \n",
    "    def remove_stop_words_lowercase_and_stem(self, tokens:List[str]) -> List[str]:\n",
    "        final_tokens = []\n",
    "        for token in tokens:\n",
    "            lowercase_token = SimplePreprocessor.lowercase_word(token)\n",
    "            if lowercase_token not in self.stop_words_set:\n",
    "                stemmed_token = self.stemmer.stem(lowercase_token)\n",
    "                final_tokens.append(stemmed_token)\n",
    "        return final_tokens\n",
    "    \n",
    "    def process_text_lines(self, text_lines:List[str]) -> List[str]:\n",
    "        tokens = self.tokenizer.tokenize_text_lines(text_lines)\n",
    "        tokens = self.remove_stop_words_lowercase_and_stem(tokens)\n",
    "        return tokens\n",
    "\n",
    "def pickle_object(obj:object, file_name:str):\n",
    "    with open(file_name, 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def unpickle_object(file_name:str) -> object:\n",
    "    with open(file_name, 'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning: must take into account the fact that some documents may disappear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the tsv file + extract the 3 corpora.\n",
    "# Assumption: 3 corpora Quran, OT, NT.\n",
    "def read_tsv_extract_corpora(tsv_file_name:str, corpus_names_to_int:Dict[str, int]) -> Dict[int, List[str]]:\n",
    "    corpora = dict()\n",
    "    for value in corpus_names_to_int.values():\n",
    "        corpora[value] = []\n",
    "    with open(tsv_file_name, mode='r', newline='\\n') as f:\n",
    "        read_tsv = csv.reader(f, delimiter=\"\\t\")\n",
    "        for row in read_tsv:\n",
    "            corpus_name = row[0]\n",
    "            corpus_id = corpus_names_to_int[corpus_name]\n",
    "            corpora[corpus_id].append(row[1])\n",
    "    return corpora\n",
    "\n",
    "def preprocess_corpora(corpora:Dict[int, List[str]], preprocessor:SimplePreprocessor) -> Dict[int, List[List[str]]]:\n",
    "    preprocessed_corpora = dict()\n",
    "    for key in corpora.keys():\n",
    "        preprocessed_corpora[key] = []\n",
    "        for document in corpora[key]:\n",
    "            document_terms = preprocessor.process_text_lines([document])\n",
    "            preprocessed_corpora[key].append(document_terms)\n",
    "    return preprocessed_corpora\n",
    "\n",
    "# ----------------------------------CREATE INDEX AND DOCID SET----------------------------------\n",
    "Index = NewType('Index', Dict[str, Dict[int, Dict[int, int]]])\n",
    "def read_corpora_and_create_index(corpora:Dict[int, List[List[str]]]) -> Tuple[Index, Dict[int, int]]:\n",
    "    \"\"\"Reads input trec file and creates a positional inverted index from it, and it also creates a set containing all document IDs.\n",
    "\n",
    "    Args:\n",
    "        input_file_name (str): input trec file name.\n",
    "        preprocessor (SimplePreprocessor): initialized SimplePreprocessor.\n",
    "    \"\"\"\n",
    "    index = dict()\n",
    "    corpora_nr_docs = dict()\n",
    "    \n",
    "    for corpus_id in corpora.keys():\n",
    "        corpora_nr_docs[corpus_id] = 0\n",
    "        for (doc_id, doc_tokens) in enumerate(corpora[corpus_id]):\n",
    "            corpora_nr_docs[corpus_id] += 1\n",
    "            for token in doc_tokens:\n",
    "                if token in index:\n",
    "                    if corpus_id in index[token]:\n",
    "                        if doc_id in index[token][corpus_id]:\n",
    "                            index[token][corpus_id][doc_id] += 1\n",
    "                        else:\n",
    "                            index[token][corpus_id][doc_id] = 1\n",
    "                    else:\n",
    "                        index[token][corpus_id] = dict()\n",
    "                        index[token][corpus_id][doc_id] = 1\n",
    "                else:\n",
    "                    index[token] = dict()\n",
    "                    index[token][corpus_id] = dict()\n",
    "                    index[token][corpus_id][doc_id] = 1\n",
    "                    \n",
    "                    \n",
    "        print(\"Index construction for corpus \" + str(corpus_id+1) + \" finished.\")\n",
    "\n",
    "    return index, corpora_nr_docs\n",
    "\n",
    "def calculate_freq_term(index:Index, term:str) -> int:\n",
    "    if term not in index:\n",
    "        return 0\n",
    "    \n",
    "    frequency = 0\n",
    "    for corpus_id in index[term]:\n",
    "        for doc_id in index[term][corpus_id]:\n",
    "            frequency += index[term][corpus_id][doc_id]\n",
    "    return frequency\n",
    "\n",
    "\n",
    "def remove_low_freq_words_from_index(corpora_index:Index, threshold_freq:int) -> Index:\n",
    "    new_index = dict()\n",
    "\n",
    "    for term in corpora_index:\n",
    "        freq = calculate_freq_term(corpora_index, term)\n",
    "        if freq >= threshold_freq:\n",
    "            new_index[term] = corpora_index[term]\n",
    "    return new_index\n",
    "    \n",
    "\n",
    "def compute_MI_score_term_corpus(N:int, N_00:int, N_01:int, N_10:int, N_11:int) -> float:\n",
    "    N_1x = N_10 + N_11 # 0 iff no corpus contains the term (impossible)\n",
    "    N_x1 = N_01 + N_11 # 0 iff the corpus doesn't contain any documents (may be possible with a cheater corpus)\n",
    "    N_0x = N_01 + N_00 # 0 iff ALL documents contain term t (may be possible if you miss a stop word or you tokenize incorrectly -- need to check for assignment imo)\n",
    "    N_x0 = N_10 + N_00 # 0 N_10 = 0 iff no other documents (from other corpora) contain the term. N_00 = 0 iff every document (from other corpora) contain the term.\n",
    "    # N_x0 can be 0 iff we have a single corpus.\n",
    "    \n",
    "    # 0 * log(0) = 0 by convention.\n",
    "    MI_score = 0\n",
    "    if N_10 != 0:\n",
    "        MI_score += (N_10/N) * np.log2((N * N_10)/(N_1x * N_x0))\n",
    "    \n",
    "    if N_01 != 0:\n",
    "        MI_score += (N_01/N) * np.log2((N * N_01)/(N_0x * N_x1))\n",
    "    \n",
    "    if N_11 != 0:\n",
    "        MI_score += (N_11/N) * np.log2((N * N_11)/(N_1x * N_x1))\n",
    "    \n",
    "    if N_00 != 0:\n",
    "        MI_score += (N_00/N) * np.log2((N * N_00)/(N_0x * N_x0))\n",
    "        \n",
    "    return MI_score\n",
    "\n",
    "def compute_chi_score_term_corpus(N:int, N_00:int, N_01:int, N_10:int, N_11:int) -> float:\n",
    "    chi_score_numerator = (N_11 + N_10 + N_01 + N_00) * (N_11 * N_00 - N_10 * N_01) ** 2\n",
    "    # Same warning as above. Term in all documents, in no document, or one-corpus dataset.\n",
    "    chi_score_denominator = (N_11 + N_01) * (N_11 + N_10) * (N_10 + N_00) * (N_01 + N_00)\n",
    "    chi_score = chi_score_numerator/chi_score_denominator\n",
    "    \n",
    "    return chi_score\n",
    "\n",
    "def compute_MI_chi_scores(index:Index, corpora_nr_docs:Dict[int, int], corpora_ids:List[int]) -> Tuple[Dict[int, List[Tuple[str, int]]], Dict[int, List[Tuple[str, int]]]]:\n",
    "    MI_scores = dict()\n",
    "    chi_scores = dict()\n",
    "\n",
    "    for corpus_id in corpora_ids:\n",
    "        MI_scores[corpus_id] = []\n",
    "        chi_scores[corpus_id] = []\n",
    "    \n",
    "    N = 0\n",
    "    for corpus_id in corpora_nr_docs:\n",
    "        N += corpora_nr_docs[corpus_id]\n",
    "    \n",
    "    nr_docs_which_contain_term = dict()\n",
    "    for term in index:\n",
    "        N_1x = 0\n",
    "        for corpus_id in index[term]:\n",
    "            N_1x += len(index[term][corpus_id])\n",
    "        nr_docs_which_contain_term[term] = N_1x\n",
    "    \n",
    "    for term in index:\n",
    "        for corpus_id in corpora_ids:\n",
    "            N_11 = 0\n",
    "            if corpus_id not in index[term]:\n",
    "                N_01 = corpora_nr_docs[corpus_id]\n",
    "            else:\n",
    "                for _ in index[term][corpus_id]:\n",
    "                    N_11 += 1\n",
    "                N_01 = corpora_nr_docs[corpus_id] - N_11\n",
    "            N_10 = nr_docs_which_contain_term[term] - N_11\n",
    "            N_00 = N - nr_docs_which_contain_term[term] - N_01\n",
    "\n",
    "            MI_scores[corpus_id].append((term, compute_MI_score_term_corpus(N, N_00, N_01, N_10, N_11)))\n",
    "            chi_scores[corpus_id].append((term, compute_chi_score_term_corpus(N, N_00, N_01, N_10, N_11)))\n",
    "    \n",
    "    for corpus_id in MI_scores:\n",
    "        MI_scores[corpus_id] = sorted(MI_scores[corpus_id], key=itemgetter(1), reverse=True)\n",
    "        chi_scores[corpus_id] = sorted(chi_scores[corpus_id], key=itemgetter(1), reverse=True)\n",
    "    return MI_scores, chi_scores\n",
    "\n",
    "def print_top_k_terms_for_each_corpus(MI_scores, chi_scores, int_to_corpus_names, k):\n",
    "    for corpus_id in MI_scores.keys():\n",
    "        corpus_name = int_to_corpus_names[corpus_id]\n",
    "        print('Top ' + str(k) + ' terms in ' + corpus_name + ' by MI score: ')\n",
    "        print(MI_scores[corpus_id][:k])\n",
    "        print('Top ' + str(k) + ' terms in ' + corpus_name + ' by Chi-squared score: ')\n",
    "        print(chi_scores[corpus_id][:k])\n",
    "        \n",
    "        file_name = corpus_name + '_' + 'MI.csv'\n",
    "        file_content = \"term,mi\\n\"\n",
    "        for (term, MI_score) in MI_scores[corpus_id][:k]:\n",
    "            file_content += term + ',' + str(round(MI_score, 5)) + '\\n'\n",
    "        with open(file_name, 'w') as f:\n",
    "            f.write(file_content)\n",
    "        \n",
    "        file_name = corpus_name + '_' + 'chi.csv'\n",
    "        file_content = \"term,chisq\\n\"\n",
    "        for (term, chi_score) in chi_scores[corpus_id][:k]:\n",
    "            file_content += term + ',' + str(round(chi_score, 3)) + '\\n'\n",
    "        with open(file_name, 'w') as f:\n",
    "            f.write(file_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_topics_task(corpora:Dict[int, List[List[str]]], corpora_nr_docs:Dict[int, int], num_topics=20):\n",
    "    clean_docs = []\n",
    "    for corpus_id in corpora:\n",
    "        clean_docs += corpora[corpus_id]\n",
    "    \n",
    "    dictionary = Dictionary(clean_docs)\n",
    "    corpus = [dictionary.doc2bow(text) for text in clean_docs]\n",
    "    lda = LdaModel(corpus, num_topics=num_topics, id2word=dictionary)\n",
    "    \n",
    "    corpora_topics_scores = dict()\n",
    "    for corpus_id in corpora:\n",
    "        corpora_topics_scores[corpus_id] = dict()\n",
    "        for ii in range(num_topics):\n",
    "            corpora_topics_scores[corpus_id][ii] = 0\n",
    "    \n",
    "    for corpus_id in corpora:\n",
    "        for doc in corpora[corpus_id]:\n",
    "            doc_topics = lda.get_document_topics(dictionary.doc2bow(doc), 0)\n",
    "            for (topic_id, topic_prob) in doc_topics:\n",
    "                corpora_topics_scores[corpus_id][topic_id] += topic_prob\n",
    "    \n",
    "    for corpus_id in corpora:\n",
    "        corpus_nr_docs = corpora_nr_docs[corpus_id]\n",
    "        for topic_id in range(num_topics):\n",
    "            corpora_topics_scores[corpus_id][topic_id] /= corpus_nr_docs\n",
    "    print(corpora_topics_scores)\n",
    "    \n",
    "    corpora_top_topic = dict()\n",
    "    for corpus_id in corpora:\n",
    "        top_topic = -1\n",
    "        top_score = 0\n",
    "        for topic_id in range(num_topics):\n",
    "            topic_score = corpora_topics_scores[corpus_id][topic_id]\n",
    "            if topic_score > top_score:\n",
    "                top_topic = topic_id\n",
    "                top_score = topic_score\n",
    "        corpora_top_topic[corpus_id] = top_topic\n",
    "    \n",
    "    for corpus_id in corpora:\n",
    "        top_topic = corpora_top_topic[corpus_id]\n",
    "        print('Top topic for corpus: ' + str(corpus_id) + \" is topic nr \" + str(top_topic))\n",
    "        print(lda.print_topic(top_topic, 10))\n",
    "    \n",
    "    for topic_id in range(num_topics):\n",
    "        print(lda.print_topic(topic_id, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index construction for corpus 1 finished.\n",
      "Index construction for corpus 2 finished.\n",
      "Index construction for corpus 3 finished.\n",
      "Top 10 terms in Quran by MI score: \n",
      "[('allah', 0.15319174926782153), ('thou', 0.03932036599419623), ('thi', 0.03125990647916953), ('ye', 0.02849049522441266), ('thee', 0.028214992863394732), ('god', 0.024976583365889097), ('man', 0.019546705009564066), ('king', 0.019286566849764573), ('hath', 0.019037355898594014), ('punish', 0.018012843149326965)]\n",
      "Top 10 terms in Quran by Chi-squared score: \n",
      "[('allah', 7058.784143738311), ('punish', 917.8365980516558), ('thou', 889.2454567342344), ('believ', 856.0119393783922), ('unbeliev', 811.8215430936408), ('messeng', 769.7409940788829), ('god', 701.8218166421327), ('thi', 699.4361927622306), ('beli', 683.3281897896653), ('guid', 677.2824079444299)]\n",
      "Top 10 terms in OT by MI score: \n",
      "[('allah', 0.08710753998082718), ('jesu', 0.04086503954395958), ('israel', 0.03613331404750586), ('lord', 0.031176664891922418), ('thi', 0.029506437544611144), ('king', 0.029178924383352338), ('thou', 0.022686042584003488), ('christ', 0.020500302127964894), ('thee', 0.018857576813994453), ('believ', 0.017334561675625193)]\n",
      "Top 10 terms in OT by Chi-squared score: \n",
      "[('allah', 2778.5750550098587), ('jesu', 1296.9727904655538), ('lord', 1118.6807985583328), ('israel', 1070.1625759380986), ('thi', 953.8911340228093), ('king', 883.6622522801713), ('thou', 776.9686315017437), ('christ', 649.0535785438349), ('thee', 633.9966008570713), ('believ', 600.444336423875)]\n",
      "Top 10 terms in NT by MI score: \n",
      "[('jesu', 0.0645836703946068), ('christ', 0.03676642588896452), ('allah', 0.01934625476268907), ('discipl', 0.018019546706597923), ('lord', 0.016068672756301375), ('ye', 0.013014132797328118), ('israel', 0.012860137279523638), ('faith', 0.01267041788681163), ('paul', 0.011852758003193985), ('peter', 0.011448916031821962)]\n",
      "Top 10 terms in NT by Chi-squared score: \n",
      "[('jesu', 3268.988776682779), ('christ', 1795.0010024059006), ('discipl', 909.799816300551), ('faith', 669.1454605103423), ('paul', 588.9450435676398), ('ye', 586.4292444979952), ('peter', 560.7510350161745), ('lord', 538.6339445755816), ('thing', 525.0495761589389), ('receiv', 490.8086096059894)]\n",
      "{0: {0: 0.04625771475331616, 1: 0.04799769776490979, 2: 0.046609182786150005, 3: 0.042712306797372326, 4: 0.05466923546839269, 5: 0.05757625795195443, 6: 0.05961817473703607, 7: 0.036459372433649904, 8: 0.043134468662036754, 9: 0.1010197107206349, 10: 0.040829378424756826, 11: 0.04300557466445216, 12: 0.029059580366324427, 13: 0.06460668158325433, 14: 0.043254307306674324, 15: 0.042739765201970745, 16: 0.06190538822820295, 17: 0.05081353443659393, 18: 0.0459886162818905, 19: 0.04174305167108829}, 1: {0: 0.03563990295285629, 1: 0.03650283905187397, 2: 0.046123613106113584, 3: 0.05251170405589571, 4: 0.05252318548512549, 5: 0.045087634099885664, 6: 0.03008393913144499, 7: 0.06458070308125523, 8: 0.04285066868735209, 9: 0.057970957276061136, 10: 0.07332025725601556, 11: 0.048451573892733114, 12: 0.07501562324015362, 13: 0.03762540398351138, 14: 0.05828804219939591, 15: 0.03580553920879129, 16: 0.04246715741200636, 17: 0.06028460081296017, 18: 0.05533116930520536, 19: 0.04953548685561837}, 2: {0: 0.05435182328250839, 1: 0.051913672453800776, 2: 0.04393562889437698, 3: 0.04929167175451322, 4: 0.04355994227264979, 5: 0.06976702361893222, 6: 0.034586950731900344, 7: 0.044029397638552834, 8: 0.04309254240827858, 9: 0.08203887019053648, 10: 0.06674409811004328, 11: 0.036045770087466907, 12: 0.03728031797344945, 13: 0.05172275932071912, 14: 0.04111832083727767, 15: 0.045528648143865785, 16: 0.05058307700166815, 17: 0.05718332254909022, 18: 0.05587638869028347, 19: 0.0413497745284444}}\n",
      "Top topic for corpus: 0 is topic nr 9\n",
      "0.095*\"god\" + 0.095*\"thing\" + 0.059*\"lord\" + 0.051*\"heaven\" + 0.040*\"earth\" + 0.037*\"world\" + 0.032*\"kingdom\" + 0.029*\"fear\" + 0.025*\"power\" + 0.019*\"bless\"\n",
      "Top topic for corpus: 1 is topic nr 12\n",
      "0.109*\"children\" + 0.097*\"israel\" + 0.058*\"offer\" + 0.047*\"lord\" + 0.027*\"sacrific\" + 0.024*\"command\" + 0.023*\"told\" + 0.023*\"measur\" + 0.021*\"fulfil\" + 0.020*\"depart\"\n",
      "Top topic for corpus: 2 is topic nr 9\n",
      "0.095*\"god\" + 0.095*\"thing\" + 0.059*\"lord\" + 0.051*\"heaven\" + 0.040*\"earth\" + 0.037*\"world\" + 0.032*\"kingdom\" + 0.029*\"fear\" + 0.025*\"power\" + 0.019*\"bless\"\n",
      "0.138*\"jesu\" + 0.047*\"beast\" + 0.038*\"open\" + 0.032*\"prophet\" + 0.027*\"blood\" + 0.026*\"gold\" + 0.023*\"prepar\" + 0.021*\"month\" + 0.020*\"spirit\" + 0.019*\"comfort\"\n",
      "0.081*\"servant\" + 0.065*\"discipl\" + 0.056*\"receiv\" + 0.048*\"walk\" + 0.048*\"jesu\" + 0.045*\"sin\" + 0.034*\"fall\" + 0.027*\"sight\" + 0.024*\"lord\" + 0.024*\"young\"\n",
      "0.076*\"voic\" + 0.039*\"put\" + 0.038*\"death\" + 0.037*\"daughter\" + 0.035*\"mountain\" + 0.034*\"mother\" + 0.030*\"son\" + 0.028*\"brother\" + 0.025*\"twelv\" + 0.020*\"trust\"\n",
      "0.059*\"flesh\" + 0.057*\"day\" + 0.049*\"hand\" + 0.041*\"life\" + 0.040*\"year\" + 0.039*\"thousand\" + 0.030*\"babylon\" + 0.028*\"end\" + 0.025*\"hundr\" + 0.022*\"begin\"\n",
      "0.054*\"cast\" + 0.040*\"land\" + 0.036*\"tree\" + 0.035*\"egypt\" + 0.030*\"part\" + 0.029*\"wit\" + 0.027*\"lord\" + 0.025*\"write\" + 0.023*\"field\" + 0.022*\"day\"\n",
      "0.075*\"father\" + 0.056*\"jesu\" + 0.037*\"deliv\" + 0.036*\"grace\" + 0.034*\"found\" + 0.029*\"turn\" + 0.027*\"gospel\" + 0.027*\"eat\" + 0.024*\"bread\" + 0.022*\"lord\"\n",
      "0.074*\"jew\" + 0.059*\"side\" + 0.053*\"chief\" + 0.046*\"cut\" + 0.039*\"merci\" + 0.034*\"way\" + 0.030*\"teach\" + 0.029*\"slain\" + 0.028*\"perish\" + 0.026*\"commit\"\n",
      "0.158*\"son\" + 0.063*\"time\" + 0.051*\"cri\" + 0.039*\"lord\" + 0.032*\"face\" + 0.029*\"feet\" + 0.029*\"tongu\" + 0.028*\"wife\" + 0.026*\"seed\" + 0.026*\"left\"\n",
      "0.121*\"behold\" + 0.082*\"work\" + 0.037*\"stood\" + 0.034*\"god\" + 0.030*\"lord\" + 0.030*\"round\" + 0.024*\"long\" + 0.023*\"manner\" + 0.023*\"good\" + 0.022*\"reign\"\n",
      "0.095*\"god\" + 0.095*\"thing\" + 0.059*\"lord\" + 0.051*\"heaven\" + 0.040*\"earth\" + 0.037*\"world\" + 0.032*\"kingdom\" + 0.029*\"fear\" + 0.025*\"power\" + 0.019*\"bless\"\n",
      "0.166*\"man\" + 0.111*\"king\" + 0.049*\"spirit\" + 0.032*\"bodi\" + 0.030*\"dead\" + 0.030*\"lord\" + 0.027*\"midst\" + 0.024*\"live\" + 0.022*\"god\" + 0.022*\"sea\"\n",
      "0.088*\"priest\" + 0.044*\"mine\" + 0.039*\"lord\" + 0.032*\"lay\" + 0.031*\"sake\" + 0.030*\"wine\" + 0.024*\"kill\" + 0.023*\"silver\" + 0.021*\"woe\" + 0.020*\"prais\"\n",
      "0.109*\"children\" + 0.097*\"israel\" + 0.058*\"offer\" + 0.047*\"lord\" + 0.027*\"sacrific\" + 0.024*\"command\" + 0.023*\"told\" + 0.023*\"measur\" + 0.021*\"fulfil\" + 0.020*\"depart\"\n",
      "0.053*\"mouth\" + 0.043*\"preach\" + 0.040*\"light\" + 0.039*\"suffer\" + 0.037*\"written\" + 0.036*\"word\" + 0.028*\"john\" + 0.025*\"dark\" + 0.024*\"lie\" + 0.022*\"hope\"\n",
      "0.105*\"hous\" + 0.055*\"men\" + 0.042*\"high\" + 0.042*\"host\" + 0.038*\"lord\" + 0.036*\"rejoic\" + 0.022*\"mine\" + 0.019*\"wall\" + 0.019*\"hand\" + 0.018*\"river\"\n",
      "0.094*\"brethren\" + 0.052*\"man\" + 0.043*\"woman\" + 0.038*\"joy\" + 0.035*\"judg\" + 0.027*\"angel\" + 0.024*\"lift\" + 0.022*\"husband\" + 0.020*\"vain\" + 0.020*\"war\"\n",
      "0.083*\"faith\" + 0.050*\"love\" + 0.043*\"righteous\" + 0.040*\"judgment\" + 0.026*\"wrath\" + 0.026*\"rich\" + 0.021*\"day\" + 0.021*\"full\" + 0.020*\"poor\" + 0.019*\"soul\"\n",
      "0.096*\"lord\" + 0.053*\"word\" + 0.047*\"holi\" + 0.036*\"law\" + 0.029*\"citi\" + 0.028*\"thing\" + 0.024*\"laid\" + 0.023*\"good\" + 0.023*\"pray\" + 0.022*\"gather\"\n",
      "0.278*\"god\" + 0.057*\"heard\" + 0.034*\"pass\" + 0.032*\"gate\" + 0.029*\"hear\" + 0.029*\"place\" + 0.026*\"peopl\" + 0.023*\"word\" + 0.021*\"rest\" + 0.020*\"cloth\"\n",
      "0.057*\"citi\" + 0.043*\"call\" + 0.038*\"set\" + 0.036*\"head\" + 0.036*\"save\" + 0.034*\"princ\" + 0.032*\"lord\" + 0.031*\"speak\" + 0.024*\"heal\" + 0.021*\"behold\"\n"
     ]
    }
   ],
   "source": [
    "tsv_file_name = 'train_and_dev.tsv'\n",
    "stopwords_file_name = \"englishST.txt\"\n",
    "index_output_file_name = \"index.txt\"\n",
    "\n",
    "stopwords_set = construct_stopwords_set(stopwords_file_name)\n",
    "tokenizer = SimpleTokenizer('[a-zA-Z]+')\n",
    "stemmer = PorterStemmer()\n",
    "preprocessor = SimplePreprocessor(tokenizer, stopwords_set, stemmer)\n",
    "\n",
    "corpus_names_to_int = {'Quran':0, 'OT':1, 'NT':2}\n",
    "int_to_corpus_names = {0:'Quran', 1:'OT', 2:'NT'}\n",
    "corpora = read_tsv_extract_corpora(tsv_file_name, corpus_names_to_int)\n",
    "\n",
    "# Apply preprocessing to the documents in the corpus.\n",
    "# Structure of \"corpora\" changes.\n",
    "corpora = preprocess_corpora(corpora, preprocessor)\n",
    "\n",
    "\n",
    "index, corpora_nr_docs = read_corpora_and_create_index(corpora)\n",
    "\n",
    "MI_scores, chi_scores = compute_MI_chi_scores(index, corpora_nr_docs, corpus_names_to_int.values())\n",
    "print_top_k_terms_for_each_corpus(MI_scores, chi_scores, int_to_corpus_names, 10)\n",
    "\n",
    "run_topics_task(corpora, corpora_nr_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_word_in_corpus(corpora, word):\n",
    "    for corpus_id in corpora:\n",
    "        occurrences = 0\n",
    "        for doc in corpora[corpus_id]:\n",
    "            for token in doc:\n",
    "                if word == token:\n",
    "                    occurrences += 1\n",
    "        print('Corpus ' + str(corpus_id) + \": \" + str(occurrences) + ' occurrences.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus 0: 2484 occurrences.\n",
      "Corpus 1: 0 occurrences.\n",
      "Corpus 2: 0 occurrences.\n"
     ]
    }
   ],
   "source": [
    "count_word_in_corpus(corpora, 'allah')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 5612, 1: 16720, 2: 5242}\n"
     ]
    }
   ],
   "source": [
    "print(corpora_nr_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
